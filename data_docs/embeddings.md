# Embeddings
Embeddings are numeric vectors representing the semantic meaning of text.
Similar meanings map to nearby vectors in embedding space.

Common use cases:
- Semantic search
- Clustering
- Reranking / retrieval
- Recommendation

Similarity:
- Cosine similarity is common. If vectors are L2-normalized, cosine similarity equals dot product.
