{"chunk_id": "chunking.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\chunking.md", "source_name": "chunking.md", "start_char": 0, "end_char": 218, "text": "﻿# Chunking\nChunking splits large documents into smaller pieces (chunks) for retrieval.\n\nKey parameters:\n- chunk_size: how big each chunk is\n- overlap: repeated text between chunks to avoid losing context at boundaries"}
{"chunk_id": "chunking.md::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\chunking.md", "source_name": "chunking.md", "start_char": 68, "end_char": 284, "text": "nks) for retrieval.\n\nKey parameters:\n- chunk_size: how big each chunk is\n- overlap: repeated text between chunks to avoid losing context at boundaries\n\nTrade-offs:\n- Smaller chunks: better precision, may lose context"}
{"chunk_id": "chunking.md::chunk_0002", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\chunking.md", "source_name": "chunking.md", "start_char": 134, "end_char": 336, "text": "unk is\n- overlap: repeated text between chunks to avoid losing context at boundaries\n\nTrade-offs:\n- Smaller chunks: better precision, may lose context\n- Larger chunks: more context, may reduce precision"}
{"chunk_id": "chunking.txt::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\chunking.txt", "source_name": "chunking.txt", "start_char": 0, "end_char": 102, "text": "﻿Chunking\nChunking splits documents into smaller parts. Overlap prevents losing context at boundaries."}
{"chunk_id": "cosine_similarity.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\cosine_similarity.md", "source_name": "cosine_similarity.md", "start_char": 0, "end_char": 207, "text": "﻿# Cosine Similarity\nCosine similarity measures the angle between two vectors.\n\ncos(u,v) = (u dot v) / (||u|| * ||v||)\n\nIf both vectors are normalized to unit length (L2-normalized),\nthen cos(u,v) = u dot v."}
{"chunk_id": "demo.txt::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\demo.txt", "source_name": "demo.txt", "start_char": 0, "end_char": 163, "text": "﻿Mini-RAG Demo Document\n\nThis project indexes documents, embeds chunks, retrieves top-k context,\nand answers questions using retrieved context (context-only mode)."}
{"chunk_id": "embeddings.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\embeddings.md", "source_name": "embeddings.md", "start_char": 0, "end_char": 237, "text": "﻿# Embeddings\nEmbeddings are numeric vectors representing the semantic meaning of text.\nSimilar meanings map to nearby vectors in embedding space.\n\nCommon use cases:\n- Semantic search\n- Clustering\n- Reranking / retrieval\n- Recommendation"}
{"chunk_id": "embeddings.md::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\embeddings.md", "source_name": "embeddings.md", "start_char": 87, "end_char": 349, "text": "Similar meanings map to nearby vectors in embedding space.\n\nCommon use cases:\n- Semantic search\n- Clustering\n- Reranking / retrieval\n- Recommendation\n\nSimilarity:\n- Cosine similarity is common. If vectors are L2-normalized, cosine similarity equals dot product."}
{"chunk_id": "embeddings.txt::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\embeddings.txt", "source_name": "embeddings.txt", "start_char": 0, "end_char": 105, "text": "﻿Embeddings\nEmbeddings are vectors representing meaning. Similarity search retrieves the closest vectors."}
{"chunk_id": "evaluation_ideas.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\evaluation_ideas.md", "source_name": "evaluation_ideas.md", "start_char": 0, "end_char": 232, "text": "﻿# Evaluation Ideas\nRAG evaluation often checks:\n- Retrieval quality: are relevant chunks in top-k?\n- Answer quality: correctness and groundedness\n- Citation quality: sources match claims\n\nSimple evaluation:\n- Create a small Q/A set"}
{"chunk_id": "evaluation_ideas.md::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\evaluation_ideas.md", "source_name": "evaluation_ideas.md", "start_char": 82, "end_char": 276, "text": "chunks in top-k?\n- Answer quality: correctness and groundedness\n- Citation quality: sources match claims\n\nSimple evaluation:\n- Create a small Q/A set\n- Measure if correct doc appears in sources"}
{"chunk_id": "failure_modes.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\failure_modes.md", "source_name": "failure_modes.md", "start_char": 0, "end_char": 220, "text": "﻿# Common Failure Modes\n- Bad chunking: important info split incorrectly\n- Embedding mismatch: wrong model used for query vs index\n- Index not rebuilt after docs update\n- Too large context: irrelevant text dilutes answer"}
{"chunk_id": "failure_modes.md::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\failure_modes.md", "source_name": "failure_modes.md", "start_char": 70, "end_char": 287, "text": "ly\n- Embedding mismatch: wrong model used for query vs index\n- Index not rebuilt after docs update\n- Too large context: irrelevant text dilutes answer\n\nMitigations:\n- tune chunk size/overlap\n- store model name in meta"}
{"chunk_id": "failure_modes.md::chunk_0002", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\failure_modes.md", "source_name": "failure_modes.md", "start_char": 137, "end_char": 311, "text": "x not rebuilt after docs update\n- Too large context: irrelevant text dilutes answer\n\nMitigations:\n- tune chunk size/overlap\n- store model name in meta\n- add logging and tests"}
{"chunk_id": "llm_in_rag.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\llm_in_rag.md", "source_name": "llm_in_rag.md", "start_char": 0, "end_char": 261, "text": "﻿# LLMs in RAG\nLLMs can be used after retrieval to produce a fluent answer using context.\nHowever, RAG can work without an LLM:\n- context-only answers can cite relevant chunks directly\n- later you can add optional LLM generation locally (e.g., ollama/llama.cpp)"}
{"chunk_id": "llm_in_rag.md::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\llm_in_rag.md", "source_name": "llm_in_rag.md", "start_char": 111, "end_char": 330, "text": "without an LLM:\n- context-only answers can cite relevant chunks directly\n- later you can add optional LLM generation locally (e.g., ollama/llama.cpp)\n\nImportant: always keep a fallback path when the LLM is unavailable."}
{"chunk_id": "ml_vs_rag.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\ml_vs_rag.md", "source_name": "ml_vs_rag.md", "start_char": 0, "end_char": 221, "text": "﻿# Classic ML vs RAG\nClassic ML classification:\n- Input: structured features (rows/columns)\n- Output: class label/probability\n\nRAG:\n- Input: unstructured documents + question\n- Output: answer grounded in retrieved sources"}
{"chunk_id": "ml_vs_rag.md::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\ml_vs_rag.md", "source_name": "ml_vs_rag.md", "start_char": 71, "end_char": 259, "text": "tures (rows/columns)\n- Output: class label/probability\n\nRAG:\n- Input: unstructured documents + question\n- Output: answer grounded in retrieved sources\n- Key: retrieval quality and evidence"}
{"chunk_id": "prompting_basics.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\prompting_basics.md", "source_name": "prompting_basics.md", "start_char": 0, "end_char": 246, "text": "﻿# Prompting Basics (for later)\nWhen using an LLM:\n- Provide retrieved context explicitly\n- Ask the model to use only that context\n- Request citations (chunk ids or source names)\n- Handle missing context: model should say 'not found in documents'"}
{"chunk_id": "prompting_basics.md::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\prompting_basics.md", "source_name": "prompting_basics.md", "start_char": 96, "end_char": 300, "text": "the model to use only that context\n- Request citations (chunk ids or source names)\n- Handle missing context: model should say 'not found in documents'\n\nThis reduces hallucination and improves reliability."}
{"chunk_id": "rag_basics.txt::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\rag_basics.txt", "source_name": "rag_basics.txt", "start_char": 0, "end_char": 231, "text": "﻿RAG (Retrieval-Augmented Generation) is a system that:\n1) Retrieves the top-k most relevant text chunks from your documents using embeddings and vector search.\n2) Uses the retrieved chunks as context to generate a grounded answer."}
{"chunk_id": "rag_basics.txt::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\rag_basics.txt", "source_name": "rag_basics.txt", "start_char": 81, "end_char": 314, "text": "st relevant text chunks from your documents using embeddings and vector search.\n2) Uses the retrieved chunks as context to generate a grounded answer.\nIf the documents do not contain the answer, the system should say it is not found."}
{"chunk_id": "rag_overview.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\rag_overview.md", "source_name": "rag_overview.md", "start_char": 0, "end_char": 231, "text": "﻿# RAG Overview\nRAG (Retrieval-Augmented Generation) answers questions by retrieving relevant text chunks from a document corpus,\nthen using those chunks as context for the answer. This reduces hallucinations and enables citations."}
{"chunk_id": "rag_overview.md::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\rag_overview.md", "source_name": "rag_overview.md", "start_char": 81, "end_char": 305, "text": "ing relevant text chunks from a document corpus,\nthen using those chunks as context for the answer. This reduces hallucinations and enables citations.\n\nPipeline:\n1) Ingest docs\n2) Chunk\n3) Embed chunks\n4) Vector search top-k"}
{"chunk_id": "rag_overview.md::chunk_0002", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\rag_overview.md", "source_name": "rag_overview.md", "start_char": 155, "end_char": 339, "text": "s context for the answer. This reduces hallucinations and enables citations.\n\nPipeline:\n1) Ingest docs\n2) Chunk\n3) Embed chunks\n4) Vector search top-k\n5) Answer using retrieved context"}
{"chunk_id": "rag_overview.txt::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\rag_overview.txt", "source_name": "rag_overview.txt", "start_char": 0, "end_char": 98, "text": "﻿RAG Overview\nRAG retrieves relevant chunks from documents and uses them as context for answering."}
{"chunk_id": "vector_search.md::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\vector_search.md", "source_name": "vector_search.md", "start_char": 0, "end_char": 225, "text": "﻿# Vector Search\nVector search retrieves top-k vectors most similar to a query vector.\nSteps:\n- Embed query text\n- Compute similarity between query embedding and document chunk embeddings\n- Return top-k chunk indices + scores"}
{"chunk_id": "vector_search.md::chunk_0001", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\vector_search.md", "source_name": "vector_search.md", "start_char": 75, "end_char": 322, "text": "ery vector.\nSteps:\n- Embed query text\n- Compute similarity between query embedding and document chunk embeddings\n- Return top-k chunk indices + scores\n\nFAISS can speed this up. If FAISS isn't available, cosine similarity can be computed via NumPy."}
{"chunk_id": "vector_search.txt::chunk_0000", "source_path": "C:\\Users\\asult\\Downloads\\opengeo_ready\\ml_RAG\\mini_rag\\data_docs\\vector_search.txt", "source_name": "vector_search.txt", "start_char": 0, "end_char": 103, "text": "﻿Vector Search\nTop-k retrieval can use FAISS or cosine similarity with NumPy when FAISS is unavailable."}
